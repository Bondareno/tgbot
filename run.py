import re
import pandas as pd
import nltk
from nltk import punkt
from nltk import *
from bs4 import BeautifulSoup
from pymystem3 import Mystem
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import pymorphy2
import seaborn as sns
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
import requests
import telebot
from telebot import *
import io
import traceback

nltk.download('punkt')
nltk.download('stopwords')
stop_words = set(stopwords.words('russian'))
sns.color_palette("tab10")
sns.set(style="whitegrid")

TOKEN='6954819921:AAHVZw4_nZ36dU7iqnucWBbsyPeP4CSgrUY'
bot = telebot.TeleBot(TOKEN)
morph = pymorphy2.MorphAnalyzer()
m = Mystem(disambiguation=False)

def extract_data(html_content):
    try:
        soup = BeautifulSoup(html_content, 'html.parser')

        text = soup.get_text()

        date_match = re.search(r'\d+\s+[A-Za-z]+\s+\d+', text)
        date = date_match.group() if date_match else None

        time_match = re.search(r'\d+:\d+', text)
        time = time_match.group() if time_match else None

        speakers = re.findall(r'‚Äî\s+([^\n]+)', text)
        dialogues = re.findall(r'‚Äî\s+([^\n]+)\n', text)

        text_extracted = speakers + dialogues
        tdf = pd.DataFrame(text_extracted)
        tdf = tdf.rename(columns={0:'text'})
    
    except FileNotFoundError:
        print("–§–∞–π–ª –ø–æ–∫–∞ –Ω–µ—Ç.")
    return tdf
    

def len_sent(tdf):
    tdf['sent_count'] = tdf['text'].apply(lambda x:len(nltk.sent_tokenize(x)))
    tdf['word_count'] = tdf['text'].apply(lambda x:len(nltk.word_tokenize(x)))
    tdf['word_per_sent'] = tdf['word_count']/tdf['sent_count']
    word_per_sent = tdf['word_per_sent'].mean()
    return round(word_per_sent) 


def plot_dirty(tdf, ax):
    count_vect_total = CountVectorizer(ngram_range=(1,1), min_df=5)

    corpus_total = [x for x in tdf['text'].fillna(' ') if len(str(x)) > 0]

    corpus_total_fit = count_vect_total.fit_transform(corpus_total)
    total_counts = pd.DataFrame(corpus_total_fit.toarray(), columns=count_vect_total.get_feature_names_out()).sum()
    ngram_total_df = pd.DataFrame(total_counts, columns=['counts'])

    
    ngram_total_df = ngram_total_df.sort_values(by='counts', ascending=False)
    
    plot_dirty = sns.barplot(x="counts",
                             y=ngram_total_df.head(20).index,
                             ax=ax,
                             data=ngram_total_df.head(20),
                             hue=ngram_total_df.head(20).index,
                             legend=False)
    
    ax.set_title('T–û–ü –°–õ–û–í –ë–ï–ó –ß–ò–°–¢–ö–ò')
    return plot_dirty

def lemmatize(wrds, m):
    res = []
    for wrd in wrds:
        p = m.parse(wrd)[0]
        res.append(p.normal_form)
        
    return res

def tokenize(text, stoplst):
    without_stop_words = []
    txxxt = nltk.word_tokenize(text)
    for word in txxxt:
        if len(word) == 1:
            continue
        if word.lower() not in stoplst:
            without_stop_words.append(word)
    return without_stop_words



def plot_clean(tdf,stop_words, ax):
    text = str(tdf['text'].values)#.astype('str')
    tokens = word_tokenize(text)
    tokens = lemmatize(tokens, morph)
    filtered_tokens = [word for word in tokens if word not in stop_words]

    count_vect_total = CountVectorizer(ngram_range=(1,2),min_df=5)
    corpus_total = [x for x in filtered_tokens if (str(x) not in stop_words) and (len(x)>3)]


    corpus_total_fit = count_vect_total.fit_transform(corpus_total)#(corpus_total)

    total_counts = pd.DataFrame(corpus_total_fit.toarray(),columns=count_vect_total.get_feature_names_out()).sum()
    ngram_total_df = pd.DataFrame(total_counts,columns=['counts'])

    ngram_total_df = ngram_total_df.sort_values(by=['counts'],ascending=False)
    
    plot_clean = sns.barplot(x="counts",
                            y=ngram_total_df.head(20).index,
                            data=ngram_total_df.head(20),
                            
                            hue=ngram_total_df.head(20).index,
                            legend=False, ax=ax)
    ax.set_title('T–û–ü –°–õ–û–í –ü–û–°–õ–ï –ß–ò–°–¢–ö–ò')
                    
    return plot_clean



def uniq_words_share(tdf,stoplst):
    txt = str(tdf['text'].values)
    tokenized_text = tokenize(txt, stop_words)
    tokens = lemmatize(tokenized_text, morph)
    words = len(tokens)
    uniq_words = len(set(tokens))
    return (uniq_words/words) * 100


def process_and_visualize(stop_words, tdf):
    text = str(tdf['text'].values)
    
    def pre_process(text):
        text = re.sub(r"</?.*?>", " <> ", str(text))
        text = re.sub(r"(\\d|\\W)+", "", text)
        text = re.sub(r'[^–∞-—è–ê-–Ø\s]+', ' ', text)
        text = text.lower()
        text = re.sub(r"\r\n", " ", text)
        text = re.sub(r"\xa0", " ", text)
        sub_str = 'sfgff'
        text = text[:text.find(sub_str)]
        return text

    morph = pymorphy2.MorphAnalyzer()

    cleaned_text = pre_process(text)
    tokenized_text = tokenize(cleaned_text, stop_words)
    lemmatized_text = lemmatize(tokenized_text, morph)

    Fdist = FreqDist(lemmatized_text)

    not_most_common = Fdist.most_common()[-21:-1]
    rare_words = [i[0] for i in not_most_common]

    

    return ', '.join(str(w) for w in rare_words)


# top pos:
def plot_pos(tdf, ax):
    pos_counter_bk = Counter()
    bk = str(tdf['text'].values)

    for sentence in sent_tokenize(bk, language="russian"):
        doc = m.analyze(sentence) 
        for word in doc: 
                if "analysis" not in word or len(word["analysis"]) == 0: 
                    continue

                gr = word["analysis"][0]['gr']
                pos = gr.split("=")[0].split(",")[0]
                pos_counter_bk[pos] += 1 

    pos_tags = []
    counts = []

    for pos, count in pos_counter_bk.most_common():
        pos_tags.append(pos)
        counts.append(count)

    percents = [count / sum(counts) * 100 for count in counts]
    bk_plt = ax.bar(pos_tags, percents, )
    ax.set_title('T–û–ü –ß–ê–°–¢–ï–ô –†–ï–ß–ò –≤ %')
    plt.xticks(rotation=30)
    
    return bk_plt



def plot_top_names(tdf, ax):
    morph = pymorphy2.MorphAnalyzer()

    def names_extr(wrds):
        res = []
        for wrd in wrds:
            p = morph.parse(wrd)[0]
            if 'Name' in p.tag:
                res.append(wrd)
        return res

    text = str(tdf['text'].values)

    without_stop_words = tokenize(text, stop_words)
    without_stop_words = lemmatize(without_stop_words, morph)
    is_name = names_extr(without_stop_words)
    Fdist = FreqDist(is_name)

    df = pd.DataFrame(list(Fdist.items()), columns=['Name', 'Count']).sort_values(by = 'Count',ascending=False)
    df = df.head(20)

    top_names_plot = sns.barplot(x="Count",
                             y="Name",
                             data=df,
                             legend=False, ax=ax)
    ax.set_title('–¢–û–ü –ò–ú–Å–ù')

    return top_names_plot


def create_plot():
    fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(18.5, 14), layout="constrained")
    return fig, axs


def plot_all_graphs(axs, html_content): #fig
    try:
        tdf = extract_data(html_content)
        if isinstance(tdf, pd.DataFrame)==False:
            if tdf==None: 
                print('tdf –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç')  
                return None
            
    except FileNotFoundError: 
        print("–§–∞–π–ª –ø–æ–∫–∞ –Ω–µ—Ç.")
    

    messages=messages = [f'–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ø–æ—Å—Ç–æ–≤ –±–ª–æ–≥–∞: {len(tdf)}', 
                         f"–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª-–≤–æ —Å–ª–æ–≤ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ ~ {len_sent(tdf)},",
                         f"–î–æ–ª—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç ~ {round(uniq_words_share(tdf,stop_words))}%",
                         f"–°–ø–∏—Å–æ–∫ 20-—Ç–∏ —Ä–µ–¥–∫–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏—Ö—Å—è —Å–ª–æ–≤: {process_and_visualize(stop_words, tdf)}"]

    plot_clean(tdf,stop_words, axs[0, 0])
    # –í—ã–∑–æ–≤–∏—Ç–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –ø–µ—Ä–µ–¥–∞–π—Ç–µ –∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –æ–±—ä–µ–∫—Ç ax
    plot_dirty(tdf, axs[0, 1])
    plot_pos(tdf, axs[1, 0])
    plot_top_names(tdf, axs[1, 1])
    #fig.tight_layout()
    
    return messages#, fig


fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(18.5, 14), layout="constrained")


def exit(exitCode):
    print(exitCode)
    print(traceback.format_exc())


@bot.message_handler(commands=['start'])
def start(message):
    bot.send_message(message.chat.id, f'–ü—Ä–∏–≤–µ—Ç, {message.from_user.first_name}!')
    change_mode(message)

def change_mode(message):
    markup = types.ReplyKeyboardMarkup(resize_keyboard=True)
    btn1 = types.KeyboardButton("–ó–∞–≥—Ä—É–∑–∏—Ç—å")
    btn2 = types.KeyboardButton("üõà –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è")
    markup.add(btn1, btn2)
    
    bot.send_message(message.chat.id, f'–ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å?', reply_markup=markup)
    bot.register_next_step_handler(message, mode_router)

def mode_router(message):
    if message.text == '–ó–∞–≥—Ä—É–∑–∏—Ç—å':
        search(message)
    elif message.text == "üõà –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è":
        send_instruction(message)
    else:
        bot.send_message(message.chat.id, '–ù–µ–≤–µ—Ä–Ω—ã–π –≤–≤–æ–¥. –í—ã–±–µ—Ä–∏—Ç–µ –æ–¥–∏–Ω –∏–∑ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ (–∫–Ω–æ–ø–æ–∫).')
        change_mode(message)

def send_instruction(message):
    try:
        with open('source/–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è —Ç–≥.pdf', 'rb') as file:
            bot.send_document(message.chat.id, file)
        change_mode(message)
    except Exception as e:
        tb = traceback.format_exc()
        print(tb)
        exit("Failed to convert name" + str(e))
        bot.send_message(message.chat.id, '–§–∞–π–ª –ø–æ–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω')
        change_mode(message)

def search(message):
    markup = types.ReplyKeyboardMarkup(resize_keyboard=True).add(types.KeyboardButton("üîô –ù–∞–∑–∞–¥"))
    bot.send_message(message.chat.id, '–û—Ç–ø—Ä–∞–≤—å—Ç–µ –º–Ω–µ –≤—ã–≥—Ä—É–∑–∫—É –∫–∞–Ω–∞–ª–∞ –∞–≤—Ç–æ—Ä–∞ –≤ –≤–∏–¥–µ html-—Ñ–∞–π–ª–∞', reply_markup=markup)
    bot.register_next_step_handler(message, step_1)

def step_1(message):
    if message.text == 'üîô –ù–∞–∑–∞–¥':
        change_mode(message)
        
    elif message.document and message.document.mime_type == 'text/html':
        process_html_file(message)
        
    else:
        bot.send_message(message.chat.id, '–ù—É–∂–Ω–æ –≤—ã—Å–ª–∞—Ç—å html-—Ñ–∞–π–ª')
        bot.register_next_step_handler(message, step_1)

def process_html_file(message):
    try:
        file_path = bot.get_file(message.document.file_id).file_path

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª –∏–∑ Telegram
        response = requests.get(f'https://api.telegram.org/file/bot{TOKEN}/{file_path}')
        html_content=response.content.decode('utf-8')
        print('–§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω –≤ –ø–∞–º—è—Ç—å')
        
        fig, axs = create_plot()  
        messages = plot_all_graphs(axs, html_content) 

        if not fig and not messages:
            bot.send_message(message.chat.id, '–ü—É—Å—Ç–æ')
            search(message)
            return

        buf = io.BytesIO()
        fig.savefig(buf, format='png')
        buf.seek(0)

        for text in messages:
            bot.send_message(message.chat.id, text)

        bot.send_photo(message.chat.id, buf)

        del html_content
        
        plt.clf()
        plt.close('all')
        change_mode(message)
        
    except Exception as e:
        tb = traceback.format_exc()
        print(tb)
        exit("Failed to convert name" + str(e))
        bot.send_message(message.chat.id, '–ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞')
        search(message)

bot.infinity_polling()
